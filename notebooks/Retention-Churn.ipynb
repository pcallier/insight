{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 as ppg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = ppg.connect(dbname=\"tweets\", user=\"patrick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con.rollback()\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"select user_id, max(tweets.created_at) as tweet_time, \"\n",
    "                \"usr.created_at, last_tweet_at, (last_tweet_at < '2015-05-15') as churn, \"\n",
    "                \"usr.followers_count, usr.friends_count from tweets inner join \"\n",
    "                \"(select user_id, name, followers_count, friends_count, last_tweet_at, created_at from users \"\n",
    "                \"where last_tweet_at is not null) as usr using (user_id) \"\n",
    "                \"where tweets.created_at < '2015-04-04' group by user_id, usr.created_at, \"\n",
    "                \"usr.last_tweet_at, usr.followers_count, usr.friends_count order by usr.created_at;\")\n",
    "    churn_df = pd.DataFrame(cur.fetchall(), columns=[c[0] for c in cur.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201809, 7)\n"
     ]
    }
   ],
   "source": [
    "churn_df.columns\n",
    "print churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n"
     ]
    }
   ],
   "source": [
    "# plot \n",
    "# training-testing\n",
    "query_date = pd.to_datetime(datetime.datetime.strptime(\"2015-06-17\", \"%Y-%m-%d\"))\n",
    "churn_df['user_age'] = (query_date - churn_df.created_at).astype(np.int64)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "training, testing = train_test_split(churn_df[['churn','followers_count','friends_count','user_age']], \n",
    "                                     train_size=0.8)\n",
    "\n",
    "dscl = StandardScaler()\n",
    "training_predictors = dscl.fit_transform(training.iloc[:,1:])\n",
    "testing_predictors = dscl.transform(testing.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161447, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "def try_model(training_predictors, training_response, testing_predictors, testing_response, the_model):\n",
    "    the_model.fit(training_predictors, training_response)\n",
    "    classfn = the_model.predict(testing_predictors)\n",
    "    print \"Number correct: \", (classfn==testing_response).sum()\n",
    "    print \"Baseline (training): \",  np.sum(training_response==True) / float(len(training_response))\n",
    "    print \"Percent correct (training): \", (the_model.predict(training_predictors)==\n",
    "                                           training_response).sum() / float(len(training_response))\n",
    "    print \"Percent correct (testing): \", (classfn==testing_response).sum() / float(len(classfn))\n",
    "    \n",
    "    print sklearn.metrics.confusion_matrix(testing_response.astype(int), classfn.astype(int))\n",
    "    print \"Precision: \", sklearn.metrics.precision_score(testing_response.astype(int), classfn.astype(int))\n",
    "    print \"Recall: \", sklearn.metrics.recall_score(testing_response.astype(int), classfn.astype(int))\n",
    "    \n",
    "    return the_model\n",
    "\n",
    "\n",
    "#clf.fit(training_predictors, training[:,0])\n",
    "#clf\n",
    "for cval in (100000,):\n",
    "    print(\"C =\", cval)\n",
    "    sample_rows = range(0,training_predictors.shape[0])\n",
    "    svm = try_model(training_predictors[sample_rows,:], training.iloc[sample_rows,0], testing_predictors, testing.iloc[:,0], SVC(C=cval,class_weight='auto'))\n",
    "    joblib.dump(svm, 'svm_{}.pkl'.format(int(time.time())))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
