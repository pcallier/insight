{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<connection object at 0x7f4da9ecdd70; dsn: 'dbname=tweets user=patrick', closed: 0>\n",
      "Number of users:  1486\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import psycopg2 as ppg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "con = ppg.connect(\"dbname=tweets user=patrick\")\n",
    "print con\n",
    "\n",
    "usa = (-125.6791025,25.4180700649,-66.885417,\n",
    "                           49.3284551525)\n",
    "la = ( -119.578941, 32.799580, -114.130814, 35.809120)\n",
    "    \n",
    "def in_place(pts, box, long_field='tweet_long', lat_field='tweet_lat'):\n",
    "    return pd.Series((pts[long_field] > box[0]) & \n",
    "                     (pts[lat_field] > box[1]) &\n",
    "                     (pts[long_field] < box[2]) &\n",
    "                     (pts[lat_field] < box[3]))\n",
    "\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT tw.tweet_id, tw.user_id, tw.lat as tweet_lat, tw.long as tweet_long, \"\n",
    "                \"tw.created_at, user_locs.followers_count,user_locs.friends_count, \"\n",
    "                \"user_locs.place as user_location, plcs.user_lat, \"\n",
    "                \"plcs.user_long \"\n",
    "                \"FROM tweets as tw, (SELECT user_id, place, friends_count, followers_count \"\n",
    "                \"FROM users) as user_locs, \"\n",
    "                \"(SELECT place, coded_lat as user_lat, coded_long as user_long FROM \"\n",
    "                \"places) as plcs \"\n",
    "                \"WHERE tw.user_id = user_locs.user_id and user_locs.place != 'None' \"\n",
    "                \"and plcs.place = user_locs.place AND plcs.user_lat != -500 AND \"\n",
    "                \"plcs.user_long != 500 AND (user_locs.place ~* '[A-Z\\s]+,\\s*[A-Z]+' OR \"\n",
    "                \"user_locs.place ~* '(New York City|NYC|LA|Los Angeles|Chicago|DC|SF|San Francisco|\"\n",
    "                \"Detroit|Houston|Dallas|Atlanta|Philly|Philadelphia|Phoenix|San Antonio| \"\n",
    "                \"San Jose|Austin|Jacksonville|Indianapolis|Denver)') AND \"\n",
    "                \"tw.text !~* '#insurancejobs|#jobs?|#hiring|#tweetmyjobs';\")\n",
    "    \n",
    "    geotags = pd.DataFrame(cur.fetchall(), \n",
    "                       columns=[c[0] for c in cur.description])\n",
    "    \n",
    "geotags['in_us'] = in_place(geotags[['tweet_long','tweet_lat']], usa)\n",
    "geotags['in_la'] = in_place(geotags[['tweet_long','tweet_lat']], la)\n",
    "\n",
    "geotags= geotags.merge(geotags.groupby('user_id').apply(lambda x: pd.Series(x.shape[0], index=['num_tweets_user'])),\n",
    "                    left_on='user_id', right_index=True, how='left')\n",
    "\n",
    "# drop certain users\n",
    "geotags = geotags[(geotags.followers_count < 2000) & \n",
    "                  (geotags.friends_count < 1800) &\n",
    "                  (geotags.num_tweets_user < 200) &\n",
    "                  (geotags.num_tweets_user >= 3) &\n",
    "                  (geotags.friends_count / geotags.followers_count >= 0.25)]\n",
    "\n",
    "# look at distance between user location and tweet location\n",
    "\n",
    "geotags['tweet_distance'] = np.sqrt((geotags.user_lat - geotags.tweet_lat) ** 2 + \n",
    "                                    (geotags.user_long - geotags.tweet_long) ** 2)\n",
    "geotags['on_road'] = geotags['tweet_distance'] > 0.5\n",
    "geotags['time_of_day'] =  geotags.created_at.map(lambda x: x.hour)\n",
    "geotags['day_of_week'] =  geotags.created_at.map(lambda x: x.weekday())\n",
    "\n",
    "\n",
    "# get number of users w/ any on-road tweets, other per-user stats\n",
    "users_on_road=geotags.groupby('user_id').apply(lambda x: pd.Series([x.on_road.any(), x.on_road.sum()],\n",
    "         index=['any_on_road','num_on_road']))\n",
    "geotags = geotags.merge(users_on_road, left_on='user_id', right_index=True)\n",
    "\n",
    "# get locations for tweets in LA\n",
    "import re\n",
    "# get number of distinct users\n",
    "print \"Number of users: \", geotags[geotags.in_la].user_id.drop_duplicates().shape[0]\n",
    "# their locations\n",
    "#print geotags[geotags.in_la][['user_id','user_location']].drop_duplicates().user_location\n",
    "def normalizer(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    txt = re.sub(ur\"[,.]\", u\" \", txt,  re.UNICODE)\n",
    "    txt = re.sub(ur\"\\s+\", u\" \", txt, re.UNICODE)\n",
    "    txt = re.sub(ur\"cali[^\\s]+\", u\"ca\", txt, re.UNICODE | re.IGNORECASE)\n",
    "    return txt\n",
    "#print geotags[geotags.in_la][['user_id','user_location']].drop_duplicates().user_location.map(normalizer).value_counts().index.tolist()\n",
    "\n",
    "# get places\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT user_id, place, coded_lat, coded_long FROM users \"\n",
    "                \"JOIN (SELECT place, coded_lat, coded_long FROM places) AS places USING (place)\")\n",
    "    places_df = pd.DataFrame(cur.fetchall(), columns = [c[0] for c in cur.description])\n",
    "places_df['places_norm'] = places_df.place.map(lambda x: x.decode('utf-8')).map(normalizer)\n",
    "places_df = places_df.merge(pd.DataFrame(geotags[geotags.in_la][['user_id','user_location']]. \\\n",
    "                                         drop_duplicates().user_location.map(lambda x: x.decode('utf-8')).\n",
    "                                         map(normalizer)),\n",
    "               left_on='places_norm',right_on='user_location')\n",
    "\n",
    "places_df['in_la'] = places_df['coded_long'].map(lambda x: (x > la[0]) & (x < la[2])) & \\\n",
    "                     places_df['coded_lat'].map(lambda x: (x > la[1]) & (x < la[3])) | \\\n",
    "                    (places_df.places_norm == \"ca\") | (places_df.places_norm == \"la\") | \\\n",
    "                    (places_df.places_norm == 'san diego ca')\n",
    "        \n",
    "# limit to ppl who tweet in LA\n",
    "geotags_la = geotags[geotags.in_la].merge(places_df, on='user_id', suffixes = ('_tweet','_user')).drop_duplicates()\n",
    "\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT tweet_id, text, user_id, tokens FROM tweets\")\n",
    "    tweets_df = pd.DataFrame(cur.fetchall(), columns=[c[0] for c in cur.description])\n",
    "la_tourist_tweets = geotags_la[geotags_la.in_la_user==False].merge(tweets_df, on='tweet_id')\n",
    "la_native_tweets = geotags_la[geotags_la.in_la_user==True].merge(tweets_df, on='tweet_id')\n",
    "\n",
    "# add tweet text\n",
    "geotags_la = geotags_la.merge(tweets_df, on='tweet_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# language modeling\n",
    "\n",
    "# TODO: think about URLs, 3-grams, apostrophes\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import ark_twokenize_py as ark\n",
    "\n",
    "def normalizer(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    txt = re.sub(ur\"[,.]\", u\" \", txt,  re.UNICODE)\n",
    "    txt = re.sub(ur\"\\s+\", u\" \", txt, re.UNICODE)\n",
    "    txt = re.sub(ur\"cali[^\\s]+\", u\"ca\", txt, re.UNICODE | re.IGNORECASE)\n",
    "    return txt\n",
    "\n",
    "def add_boundaries(tkns):\n",
    "    return [\"#\"] + tkns + [\"#\"]\n",
    "\n",
    "def get_ngram_model(stuff, n=2):\n",
    "    my_ngrams = ngrams([tkn for tkns in map(normalizer, stuff) for tkn in add_boundaries(tkns.split())], n)\n",
    "    return dict(Counter(my_ngrams))\n",
    "\n",
    "def ngram_inference(ngram_dict, new_string, n=2):\n",
    "    \"\"\"Don't try to scale this above n=2 yet\"\"\"\n",
    "    if n==2:\n",
    "        ngram_dict[('#','#')] = 0\n",
    "    num_tokens = np.sum(list(ngram_dict.itervalues()))\n",
    "    \n",
    "    \n",
    "    # tokenize new string\n",
    "    new_tokens = ark.tokenize(normalizer(new_string))\n",
    "    if n==2:\n",
    "        new_tokens = add_boundaries(new_tokens)\n",
    "    \n",
    "    # get bigrams\n",
    "    new_ngrams = ngrams(new_tokens, n)\n",
    "    # get counts\n",
    "    new_counts = { k : ngram_dict.get(k, 1) for k in new_ngrams }\n",
    "    #print new_counts\n",
    "    #print num_tokens\n",
    "    # counts to frequencies\n",
    "    new_freqs = [ float(v)/float(num_tokens) for v in new_counts.itervalues() ]\n",
    "    \n",
    "    # string probability\n",
    "    return np.prod(new_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.88776211808e-26\n",
      "9.17278857416e-22\n",
      "9.88776211808e-26\n",
      "9.17278857416e-22\n",
      "9.88776211808e-26\n",
      "9.17278857416e-22\n"
     ]
    }
   ],
   "source": [
    "stuff = [u\"hello there okay\"]\n",
    "\n",
    "ngram_dict = get_ngram_model(la_native_tweets.tokens.dropna().map(\n",
    "            lambda x: x.decode('utf-8')), n=1)\n",
    "ngram_dict_tour = get_ngram_model(la_tourist_tweets.tokens.dropna().map(\n",
    "            lambda x: x.decode('utf-8')), n=1)\n",
    "print ngram_inference(ngram_dict, \"etchings of unknown arbiters\")\n",
    "print ngram_inference(ngram_dict_tour, \"etchings of unknown arbiters\")\n",
    "print ngram_inference(ngram_dict, \"im at hollywood blvd\")\n",
    "print ngram_inference(ngram_dict_tour, \"im at hollywood blvd\")\n",
    "print ngram_inference(ngram_dict, \"you are so awful\")\n",
    "print ngram_inference(ngram_dict_tour, \"you are so awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try some inference\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "training, testing = train_test_split(geotags_la[['in_la_user', 'tokens', 'time_of_day', 'day_of_week']].dropna(), \n",
    "                                     random_state=555, train_size=0.8)\n",
    "\n",
    "training[:,1] = map(lambda x: x.decode('utf-8'), training[:,1])\n",
    "testing[:,1] = map(lambda x: x.decode('utf-8'), testing[:,1])\n",
    "#print training[0,0]\n",
    "# train lg model features\n",
    "n=1\n",
    "training_model_tourist = get_ngram_model(training[training[:,0]==False,n])\n",
    "training_model_native = get_ngram_model(training[training[:,0]==True,n])\n",
    "\n",
    "# add probabilities from lg models\n",
    "training_probs_tourist = np.array([ngram_inference(training_model_tourist,x,n) for x in training[:,1]], ndmin=2).T\n",
    "training_probs_native = np.array([ngram_inference(training_model_native,x,n) for x in training[:,1]], ndmin=2).T\n",
    "testing_probs_tourist = np.array([ngram_inference(training_model_tourist,x,n) for x in testing[:,1]], ndmin=2).T\n",
    "testing_probs_native = np.array([ngram_inference(training_model_native,x,n) for x in testing[:,1]], ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3.3149618698275607e-91]\n",
      " [6 3 3.426322408368742e-54]\n",
      " [20 0 4.297185071857467e-87]\n",
      " [0 2 2.7316502014968842e-21]\n",
      " [20 2 9.36053392145069e-75]\n",
      " [18 1 1.2134060122376188e-70]\n",
      " [7 3 2.557248993155566e-95]\n",
      " [4 3 5.570441008648835e-83]\n",
      " [23 3 7.22096267951148e-79]\n",
      " [18 0 7.463538622167737e-42]]\n"
     ]
    }
   ],
   "source": [
    "training_more = np.concatenate([training, training_probs_tourist - training_probs_native], axis=1)\n",
    "testing_more = np.concatenate([testing, testing_probs_tourist - testing_probs_native], axis=1)\n",
    "\n",
    "# extract and scale predictors\n",
    "print training_more[:10,2:]\n",
    "training_scaler = StandardScaler()\n",
    "training_predictors = training_scaler.fit_transform(training_more[:,2:])\n",
    "testing_predictors = training_scaler.transform(testing_more[:,2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number correct:  1168\n",
      "Baseline (training):  0.859409594096\n",
      "Percent correct (training):  0.859409594096\n",
      "Percent correct (testing):  0.861992619926\n",
      "[[   0  187]\n",
      " [   0 1168]]\n"
     ]
    }
   ],
   "source": [
    "# some kinda machine learning\n",
    "\n",
    "# Naive Bayes, Random Forest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def try_model(training_predictors, training_response, testing_predictors, testing_response, the_model):\n",
    "    the_model.fit(training_predictors, training_response)\n",
    "    classfn = the_model.predict(testing_predictors)\n",
    "    print \"Number correct: \", (classfn==testing_response).sum()\n",
    "    print \"Baseline (training): \",  np.sum(training_response==True) / float(len(training_response))\n",
    "    print \"Percent correct (training): \", (the_model.predict(training_predictors)==\n",
    "                                           training_response).sum() / float(len(training_response))\n",
    "    print \"Percent correct (testing): \", (classfn==testing_response).sum() / float(len(classfn))\n",
    "    \n",
    "    print sklearn.metrics.confusion_matrix(testing_response.astype(int), classfn.astype(int))\n",
    "\n",
    "\n",
    "# try naid baxes)\")\n",
    "try_model(training_predictors, training[:,0], testing_predictors, testing[:,0], SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80503915, -1.66761238],\n",
       "       [ 0.42626845,  0.54318072],\n",
       "       [ 0.93129605,  0.54318072],\n",
       "       [ 1.05755295, -1.66761238],\n",
       "       [ 0.30001155, -1.66761238],\n",
       "       [-1.46758504,  0.54318072],\n",
       "       [ 1.05755295, -1.66761238],\n",
       "       [-1.21507124,  1.28011175],\n",
       "       [ 1.05755295, -0.19375032],\n",
       "       [ 0.67878225,  1.28011175]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_predictors[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
