{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<connection object at 0x7f6c65244e88; dsn: 'dbname=tweets user=patrick', closed: 0>\n",
      "Number of users: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import psycopg2 as ppg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "con = ppg.connect(\"dbname=tweets user=patrick\")\n",
    "print con\n",
    "\n",
    "usa = (-125.6791025,25.4180700649,-66.885417,\n",
    "                           49.3284551525)\n",
    "la = ( -119.578941, 32.799580, -114.130814, 35.809120)\n",
    "    \n",
    "def in_place(pts, box, long_field='tweet_long', lat_field='tweet_lat'):\n",
    "    return pd.Series((pts[long_field] > box[0]) & \n",
    "                     (pts[lat_field] > box[1]) &\n",
    "                     (pts[long_field] < box[2]) &\n",
    "                     (pts[lat_field] < box[3]))\n",
    "\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT tw.tweet_id, tw.user_id, tw.lat as tweet_lat, tw.long as tweet_long, \"\n",
    "                \"tw.created_at, user_locs.followers_count,user_locs.friends_count, \"\n",
    "                \"user_locs.place as user_location, plcs.user_lat, \"\n",
    "                \"plcs.user_long \"\n",
    "                \"FROM tweets as tw, (SELECT user_id, place, friends_count, followers_count \"\n",
    "                \"FROM users) as user_locs, \"\n",
    "                \"(SELECT place, coded_lat as user_lat, coded_long as user_long FROM \"\n",
    "                \"places) as plcs \"\n",
    "                \"WHERE tw.user_id = user_locs.user_id and user_locs.place != 'None' \"\n",
    "                \"and plcs.place = user_locs.place AND plcs.user_lat != -500 AND \"\n",
    "                \"plcs.user_long != 500 AND (user_locs.place ~* '[A-Z\\s]+,\\s*[A-Z]+' OR \"\n",
    "                \"user_locs.place ~* '(New York City|NYC|LA|Los Angeles|Chicago|DC|SF|San Francisco|\"\n",
    "                \"Detroit|Houston|Dallas|Atlanta|Philly|Philadelphia|Phoenix|San Antonio| \"\n",
    "                \"San Jose|Austin|Jacksonville|Indianapolis|Denver)') AND \"\n",
    "                \"tw.text !~* '#insurancejobs|#jobs?|#hiring|#tweetmyjobs';\")\n",
    "    \n",
    "    geotags = pd.DataFrame(cur.fetchall(), \n",
    "                       columns=[c[0] for c in cur.description])\n",
    "    \n",
    "geotags['in_us'] = in_place(geotags[['tweet_long','tweet_lat']], usa)\n",
    "geotags['in_la'] = in_place(geotags[['tweet_long','tweet_lat']], la)\n",
    "\n",
    "geotags= geotags.merge(geotags.groupby('user_id').apply(lambda x: pd.Series(x.shape[0], index=['num_tweets_user'])),\n",
    "                    left_on='user_id', right_index=True, how='left')\n",
    "\n",
    "# drop certain users\n",
    "geotags = geotags[(geotags.followers_count < 2000) & \n",
    "                  (geotags.friends_count < 1800) &\n",
    "                  (geotags.num_tweets_user < 200) &\n",
    "                  (geotags.num_tweets_user >= 3) &\n",
    "                  (geotags.friends_count / geotags.followers_count >= 0.25)]\n",
    "\n",
    "# look at distance between user location and tweet location\n",
    "\n",
    "geotags['tweet_distance'] = np.sqrt((geotags.user_lat - geotags.tweet_lat) ** 2 + \n",
    "                                    (geotags.user_long - geotags.tweet_long) ** 2)\n",
    "geotags['on_road'] = geotags['tweet_distance'] > 0.5\n",
    "geotags['time_of_day'] =  geotags.created_at.map(lambda x: x.hour)\n",
    "geotags['day_of_week'] =  geotags.created_at.map(lambda x: x.weekday())\n",
    "\n",
    "\n",
    "# get number of users w/ any on-road tweets, other per-user stats\n",
    "users_on_road=geotags.groupby('user_id').apply(lambda x: pd.Series([x.on_road.any(), x.on_road.sum()],\n",
    "         index=['any_on_road','num_on_road']))\n",
    "geotags = geotags.merge(users_on_road, left_on='user_id', right_index=True)\n",
    "\n",
    "# get locations for tweets in LA\n",
    "import re\n",
    "# get number of distinct users\n",
    "print \"Number of users: \", geotags[geotags.in_la].user_id.drop_duplicates().shape[0]\n",
    "# their locations\n",
    "#print geotags[geotags.in_la][['user_id','user_location']].drop_duplicates().user_location\n",
    "def normalizer(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    txt = re.sub(ur\"[,.]\", u\" \", txt,  re.UNICODE)\n",
    "    txt = re.sub(ur\"\\s+\", u\" \", txt, re.UNICODE)\n",
    "    txt = re.sub(ur\"cali[^\\s]+\", u\"ca\", txt, re.UNICODE | re.IGNORECASE)\n",
    "    return txt\n",
    "#print geotags[geotags.in_la][['user_id','user_location']].drop_duplicates().user_location.map(normalizer).value_counts().index.tolist()\n",
    "\n",
    "# get places\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT user_id, place, coded_lat, coded_long FROM users \"\n",
    "                \"JOIN (SELECT place, coded_lat, coded_long FROM places) AS places USING (place)\")\n",
    "    places_df = pd.DataFrame(cur.fetchall(), columns = [c[0] for c in cur.description])\n",
    "places_df['places_norm'] = places_df.place.map(lambda x: x.decode('utf-8')).map(normalizer)\n",
    "places_df = places_df.merge(pd.DataFrame(geotags[geotags.in_la][['user_id','user_location']]. \\\n",
    "                                         drop_duplicates().user_location.map(lambda x: x.decode('utf-8')).\n",
    "                                         map(normalizer)),\n",
    "               left_on='places_norm',right_on='user_location')\n",
    "\n",
    "places_df['in_la'] = places_df['coded_long'].map(lambda x: (x > la[0]) & (x < la[2])) & \\\n",
    "                     places_df['coded_lat'].map(lambda x: (x > la[1]) & (x < la[3])) | \\\n",
    "                    (places_df.places_norm == \"ca\") | (places_df.places_norm == \"la\") | \\\n",
    "                    (places_df.places_norm == 'san diego ca')\n",
    "        \n",
    "# limit to ppl who tweet in LA\n",
    "geotags_la = geotags[geotags.in_la].merge(places_df, on='user_id', suffixes = ('_tweet','_user')).drop_duplicates()\n",
    "print geotags_la.shape\n",
    "print places_df.shape\n",
    "\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"SELECT tweet_id, text, user_id, tokens FROM tweets\")\n",
    "    tweets_df = pd.DataFrame(cur.fetchall(), columns=[c[0] for c in cur.description])\n",
    "la_tourist_tweets = geotags_la[geotags_la.in_la_user==False].merge(tweets_df, on='tweet_id')\n",
    "la_native_tweets = geotags_la[geotags_la.in_la_user==True].merge(tweets_df, on='tweet_id')\n",
    "\n",
    "# add tweet text\n",
    "geotags_la = geotags_la.merge(tweets_df, on='tweet_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# language modeling\n",
    "\n",
    "# TODO: think about URLs, 3-grams, apostrophes\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import ark_twokenize_py as ark\n",
    "\n",
    "def normalizer(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    txt = re.sub(ur\"[,.]\", u\" \", txt,  re.UNICODE)\n",
    "    txt = re.sub(ur\"\\s+\", u\" \", txt, re.UNICODE)\n",
    "    txt = re.sub(ur\"cali[^\\s]+\", u\"ca\", txt, re.UNICODE | re.IGNORECASE)\n",
    "    return txt\n",
    "\n",
    "def add_boundaries(tkns):\n",
    "    return [\"#\"] + tkns + [\"#\"]\n",
    "\n",
    "def get_ngram_model(stuff, n=2):\n",
    "    my_ngrams = ngrams([tkn for tkns in map(normalizer, stuff) for tkn in add_boundaries(tkns.split())], n)\n",
    "    return dict(Counter(my_ngrams))\n",
    "\n",
    "def ngram_inference(ngram_dict, new_string, n=2):\n",
    "    \"\"\"Don't try to scale this above n=2 yet\"\"\"\n",
    "    if n==2:\n",
    "        ngram_dict[('#','#')] = 0\n",
    "    num_tokens = np.sum(list(ngram_dict.itervalues()))\n",
    "    \n",
    "    \n",
    "    # tokenize new string\n",
    "    new_tokens = ark.tokenize(normalizer(new_string))\n",
    "    if n==2:\n",
    "        new_tokens = add_boundaries(new_tokens)\n",
    "    \n",
    "    # get bigrams\n",
    "    new_ngrams = ngrams(new_tokens, n)\n",
    "    # get counts\n",
    "    new_counts = { k : ngram_dict.get(k, 1) for k in new_ngrams }\n",
    "    #print new_counts\n",
    "    #print num_tokens\n",
    "    # counts to frequencies\n",
    "    new_freqs = [ float(v)/float(num_tokens) for v in new_counts.itervalues() ]\n",
    "    \n",
    "    # string probability\n",
    "    return np.prod(new_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.63320998613e-26\n",
      "8.69884659852e-22\n",
      "8.63320998613e-26\n",
      "8.69884659852e-22\n",
      "8.63320998613e-26\n",
      "8.69884659852e-22\n"
     ]
    }
   ],
   "source": [
    "stuff = [u\"hello there okay\"]\n",
    "\n",
    "ngram_dict = get_ngram_model(la_native_tweets.tokens.dropna().map(\n",
    "            lambda x: x.decode('utf-8')), n=1)\n",
    "ngram_dict_tour = get_ngram_model(la_tourist_tweets.tokens.dropna().map(\n",
    "            lambda x: x.decode('utf-8')), n=1)\n",
    "print ngram_inference(ngram_dict, \"etchings of unknown arbiters\")\n",
    "print ngram_inference(ngram_dict_tour, \"etchings of unknown arbiters\")\n",
    "print ngram_inference(ngram_dict, \"im at hollywood blvd\")\n",
    "print ngram_inference(ngram_dict_tour, \"im at hollywood blvd\")\n",
    "print ngram_inference(ngram_dict, \"you are so awful\")\n",
    "print ngram_inference(ngram_dict_tour, \"you are so awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2ffc34dab892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# recode categorical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mohe_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtime_of_day_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mtime_of_day_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/patrick/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \"\"\"\n\u001b[0;32m   1053\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[1;32m-> 1054\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/patrick/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \"\"\"\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matleast2d_or_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/patrick/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36m_fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X needs to contain only non-negative integers.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_values\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mn_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "# try some inference\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "training, testing = train_test_split(geotags_la[['in_la_user', 'tokens', 'time_of_day', 'day_of_week']].dropna(), \n",
    "                                     random_state=555, train_size=0.8)\n",
    "\n",
    "training[:,1] = map(lambda x: x.decode('utf-8'), training[:,1])\n",
    "testing[:,1] = map(lambda x: x.decode('utf-8'), testing[:,1])\n",
    "\n",
    "# recode categorical features\n",
    "ohe_time = OneHotEncoder()\n",
    "time_of_day_train = ohe_time.fit_transform(training[:,2])\n",
    "time_of_day_test = ohe_time.transform(testing[:,2])\n",
    "\n",
    "#print training[0,0]\n",
    "# train lg model features\n",
    "n=1\n",
    "training_model_tourist = get_ngram_model(training[training[:,0]==False,n])\n",
    "training_model_native = get_ngram_model(training[training[:,0]==True,n])\n",
    "\n",
    "# add probabilities from lg models\n",
    "training_probs_tourist = np.array([ngram_inference(training_model_tourist,x,n) for x in training[:,1]], ndmin=2).T\n",
    "training_probs_native = np.array([ngram_inference(training_model_native,x,n) for x in training[:,1]], ndmin=2).T\n",
    "testing_probs_tourist = np.array([ngram_inference(training_model_tourist,x,n) for x in testing[:,1]], ndmin=2).T\n",
    "testing_probs_native = np.array([ngram_inference(training_model_native,x,n) for x in testing[:,1]], ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3.3149618698275607e-91]\n",
      " [6 3 3.426322408368742e-54]\n",
      " [20 0 4.297185071857467e-87]\n",
      " [0 2 2.7316502014968842e-21]\n",
      " [20 2 9.36053392145069e-75]\n",
      " [18 1 1.2134060122376188e-70]\n",
      " [7 3 2.557248993155566e-95]\n",
      " [4 3 5.570441008648835e-83]\n",
      " [23 3 7.22096267951148e-79]\n",
      " [18 0 7.463538622167737e-42]]\n"
     ]
    }
   ],
   "source": [
    "training_more = np.concatenate([training, training_probs_tourist - training_probs_native], axis=1)\n",
    "testing_more = np.concatenate([testing, testing_probs_tourist - testing_probs_native], axis=1)\n",
    "\n",
    "# extract and scale predictors\n",
    "print training_more[:10,2:]\n",
    "training_scaler = StandardScaler()\n",
    "training_predictors = training_scaler.fit_transform(training_more[:,2:])\n",
    "testing_predictors = training_scaler.transform(testing_more[:,2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number correct:  1168\n",
      "Baseline (training):  0.859409594096\n",
      "Percent correct (training):  0.859409594096\n",
      "Percent correct (testing):  0.861992619926\n",
      "[[   0  187]\n",
      " [   0 1168]]\n"
     ]
    }
   ],
   "source": [
    "# some kinda machine learning\n",
    "\n",
    "# Naive Bayes, Random Forest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "def try_model(training_predictors, training_response, testing_predictors, testing_response, the_model):\n",
    "    the_model.fit(training_predictors, training_response)\n",
    "    classfn = the_model.predict(testing_predictors)\n",
    "    print \"Number correct: \", (classfn==testing_response).sum()\n",
    "    print \"Baseline (training): \",  np.sum(training_response==True) / float(len(training_response))\n",
    "    print \"Percent correct (training): \", (the_model.predict(training_predictors)==\n",
    "                                           training_response).sum() / float(len(training_response))\n",
    "    print \"Percent correct (testing): \", (classfn==testing_response).sum() / float(len(classfn))\n",
    "    \n",
    "    print sklearn.metrics.confusion_matrix(testing_response.astype(int), classfn.astype(int))\n",
    "\n",
    "\n",
    "# try naid baxes)\")\n",
    "try_model(training_predictors, training[:,0], testing_predictors, testing[:,0], SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80503915, -1.66761238],\n",
       "       [ 0.42626845,  0.54318072],\n",
       "       [ 0.93129605,  0.54318072],\n",
       "       [ 1.05755295, -1.66761238],\n",
       "       [ 0.30001155, -1.66761238],\n",
       "       [-1.46758504,  0.54318072],\n",
       "       [ 1.05755295, -1.66761238],\n",
       "       [-1.21507124,  1.28011175],\n",
       "       [ 1.05755295, -0.19375032],\n",
       "       [ 0.67878225,  1.28011175]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_predictors[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do some topic modeling\n",
    "def do_lda(corpus_strings, min_freq = 10):\n",
    "    #tokenized = [ark.tokenize(tw.decode('utf-8')) for tw in corpus_strings]\n",
    "    #normalized = [normalize_tokens(tkns) for tkns in tokenized]\n",
    "    censored = [remove_stop_words(tokens.split()) for tokens in corpus_strings]\n",
    "    corpus_string = u'\\n'.join([u' '.join(censored_doc) for censored_doc in censored])\n",
    "    doccounts = Counter(corpus_string.split())\n",
    "    doccounts = dict([(i, doccounts[i]) for i in doccounts.iterkeys() if doccounts[i] > min_freq])\n",
    "    dictionary = corpora.Dictionary(censored)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in censored]\n",
    "    lda = LdaModel(corpus, num_topics = 10, id2word = dictionary)\n",
    "    return dictionary, lda\n",
    "\n",
    "def display_lda(model, vocab, n_top_words = 8, n_top_topics=3):\n",
    "    topic_word = model.topic_word_\n",
    "    # loop over fitted probabilities for vocab items in each topic\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        # get top words for topic, having sorted by ascending probability\n",
    "        topic_words = vocab[np.argsort(topic_dist)][:-n_top_words-1:-1]\n",
    "        print u'Topic {}: {}'.format(i, u' '.join(topic_words)) \n",
    "    for i, doc_dist in enumerate(model.doc_topic_):\n",
    "        # get top topics per document\n",
    "        print \"Document \", i, \":\"\n",
    "        topic_docs = np.argsort(doc_dist)[:n_top_topics-1:-1]\n",
    "        for k in xrange(0, n_top_topics):\n",
    "            print np.argsort(doc_dist)[-k]\n",
    "            print vocab[np.argsort(model.topic_word_[np.argsort(doc_dist)[-k]])][:-n_top_words:-1]\n",
    "\n",
    "def remove_stop_words(tokens, stop_list=stopwords.words('english') + stop_punct):\n",
    "    return [tkn for tkn in tokens if tkn not in stop_list]\n",
    "    \n",
    "def tokenize_normalize_stop(txt):\n",
    "    return remove_stop_words(normalize_tokens(ark.tokenize(txt)))\n",
    "\n",
    "def doctopic_to_features(con, corpus, model):\n",
    "    \"\"\"Create matrix of features for each document (author) with \n",
    "    n columns where n is the number of topics, valued as the probability \n",
    "    of that topic for that author (first column will be doc/author id).\n",
    "    \n",
    "    Write that to the given connection. Table users\n",
    "    must have fields topic_0 to topic_(n-1) where n = no. of topics - 1\n",
    "    \n",
    "    Corpus must be a pandas series whose index is the user id\"\"\"\n",
    "    \n",
    "    #feature_matrix = np.concatenate([np.array(corpus.index, ndmin=2).T, \n",
    "    #                                 model.doc_topic_], axis=1)\n",
    "    \n",
    "    with con.cursor() as cur:\n",
    "        for user_id, doc_fragment in zip(corpus.index, corpus):\n",
    "            topic_weights = model[model.id2word.doc2bow(doc_fragment.split())]\n",
    "            set_statements =  ', '.join([\"topic_{} = {}\".format(x,y) \n",
    "                for x,y in topic_weights])\n",
    "            #logging.debug(set_statements)\n",
    "            query = \"UPDATE users SET \" + \\\n",
    "                set_statements + \\\n",
    "                \" WHERE user_id='{}'\".format(user_id)\n",
    "            cur.execute(query)\n",
    "            con.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
